{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f4984-2b54-4886-a5a2-ee7805df467d",
   "metadata": {},
   "source": [
    "#### Code to download and separate temperature and geopotential height data.\n",
    "\n",
    "So the previous code that I wrote for this needed to be modified to account for the fact that I will more than likely need to uh ... weight by latitude. I am a dummy and forgor to consider this. So this re-attempt at code adds in the saving of lat/lon dimensions. \n",
    "I have the relevant years and dates of temp/gph, yes, but I need to actually go in now and create the value arrays that distinguish my actual desired POR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d2e30d-e0e9-4474-9392-a58069d8437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/knight/anaconda_jan21/envs/aug21/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#import statements... I think these are all of the relevant ones to what I am doing here. \n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "import netCDF4\n",
    "import os\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2493bdd-1e53-44be-b6ed-b967f6d3bcd6",
   "metadata": {},
   "source": [
    "I'm going to loop through gph and temp dates to create arrays of values for the levels and whatnot that I am looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f606856c-37d8-4dfd-8a85-44fc23be59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [i for i in range(1997,2022)] #indicate years for data, needed for opening files\n",
    "#year = [i for i in range(1999,2023)] \n",
    "#index = [i for i in range(1,43)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc47aa7e-9541-4321-acfe-58ce19ee9224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1d414e-18cb-455d-9c2d-f56680609322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays for the years of data\n",
    "gph_1000 = np.empty((len(year),152,361,1440))\n",
    "gph_1000[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077b699-6eeb-44a0-8bc0-ecfcd4a9af02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#this is my attempt at looping through gph and combining the files lol\n",
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    ##1959\n",
    "    #if year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "    #if year[i] == 1959 or year[i] == 1963 or year[i] == 1967 or year[i] == 1971 or year[i] == 1975 or year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995:\n",
    "\n",
    "    ###1979\n",
    "    if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995:\n",
    "    #if year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "    #if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        gfile1 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"z\"] \n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        time_coord1 = g_data1.time.values\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"z\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        time_coord2 = g_data2.time.values\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        gph_1000[i,:61,:,:] = values1[90:,:,:]\n",
    "        gph_1000[i,61:,:,:] = values2[:91,:,:] \n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    ###1959\n",
    "    #if year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "    #if  year[i] == 1960 or  year[i] == 1964 or  year[i] == 1968 or  year[i] == 1972 or  year[i] == 1976 or year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996:\n",
    "\n",
    "    ###1979\n",
    "    if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996:\n",
    "    #if year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "    #if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        gfile1 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"z\"] #next line will reduce this to the mean \n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"z\"] #next line will reduce this to the mean\n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        gph_1000[i,:61,:,:] = values1[91:,:,:]\n",
    "        gph_1000[i,61:120,:,:] = values2[:59,:,:]\n",
    "        gph_1000[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue\n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"z\"] #next line will reduce this to the mean over th\n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "\n",
    "        gfile2 = xr.open_dataset(\"../era5/gph/era5_gph_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"z\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "        \n",
    "        gph_1000[i,:61,:,:] = values1[90:,:,:]\n",
    "        gph_1000[i,61:120,:,:] = values2[:59,:,:]\n",
    "        gph_1000[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06d19ef-2a09-4e15-8943-a2303fa0b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/knight/anaconda_jan21/envs/aug21/lib/python3.8/site-packages/xarray/backends/plugins.py:61: RuntimeWarning: Engine 'rasterio' loading failed:\n",
      "(numpy 1.24.3 (/nfs/home11/grad/2020/ef935217/.local/lib/python3.8/site-packages), Requirement.parse('numpy<1.23.0,>=1.16.5'), {'scipy'})\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n"
     ]
    }
   ],
   "source": [
    "#this is my attempt at looping through t and combining the files lol\n",
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    ##1959\n",
    "    if year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "    #if year[i] == 1959 or year[i] == 1963 or year[i] == 1967 or year[i] == 1971 or year[i] == 1975 or year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995:\n",
    "\n",
    "    ###1979\n",
    "    #if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995:\n",
    "    #if year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "    #if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] \n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        time_coord1 = g_data1.time.values\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        time_coord2 = g_data2.time.values\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        gph_1000[i,:61,:,:] = values1[90:,:,:]\n",
    "        gph_1000[i,61:,:,:] = values2[:91,:,:] \n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    ###1959\n",
    "    if year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "    #if  year[i] == 1960 or  year[i] == 1964 or  year[i] == 1968 or  year[i] == 1972 or  year[i] == 1976 or year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996:\n",
    "\n",
    "    ###1979\n",
    "    #if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996:\n",
    "    #if year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "    #if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] #next line will reduce this to the mean \n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean\n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        gph_1000[i,:61,:,:] = values1[91:,:,:]\n",
    "        gph_1000[i,61:120,:,:] = values2[:59,:,:]\n",
    "        gph_1000[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue\n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] #next line will reduce this to the mean over th\n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "        \n",
    "        gph_1000[i,:61,:,:] = values1[90:,:,:]\n",
    "        gph_1000[i,61:120,:,:] = values2[:59,:,:]\n",
    "        gph_1000[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40164b3-7e90-4982-930f-c9b6f1c6fa24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##print to check\n",
    "#gph_1000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "610e5f0c-2dae-4439-a5e9-ef0689f0cd01",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##do not forget to divide by gravity.\n",
    "gph1000 = gph_1000/9.81\n",
    "gph1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53c08ba-a2de-40ba-9742-14ca42c2bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 152, 361, 1440)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gph_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4062df68-7e24-4fbb-a3ba-86d1229df813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gph_1000, open(\"FullNH_100_temp_1959_PT2.p\", 'wb')) #download was changed because temperature\n",
    "#i also did NOT divide by 10 here to conserve memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264382d2-9ba1-4cc1-b1c6-72f945aae549",
   "metadata": {},
   "source": [
    "Okay, great. This should represent an xarray of my gph data from ERA5 at 1000hPa. It's for the full, uh ... NH.\n",
    "\n",
    "The cells below this are going to be for temperature and meridional wind. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48283e-8039-4dc4-aeae-b6b252d773f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays for the years of data\n",
    "NH_temp_100 = np.empty((44,152,361,1440))\n",
    "NH_temp_100[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fe6d3-8c62-4fe1-80ca-7fee4c8293a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Full NH temp\n",
    "for i in range(len(year)):\n",
    "    #print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        tfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        t_files1 = tfile1[\"t\"] \n",
    "        t_data1 = t_files1.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values1 =  t_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        tfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        t_files2 = tfile2[\"t\"] \n",
    "        t_data2 = t_files2.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values2 =  t_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        NH_temp_100[i,:61,:,:] = values1[90:,:,:]\n",
    "        NH_temp_100[i,61:,:,:] = values2[:91,:,:]\n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        #file 1 for start of year\n",
    "        tfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        t_files1 = tfile1[\"t\"] \n",
    "        t_data1 = t_files1.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values1 =  t_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        tfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        t_files2 = tfile2[\"t\"] #next line will reduce this to the mean\n",
    "        t_data2 = t_files2.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values2 =  t_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        NH_temp_100[i,:61,:,:] = values1[91:,:,:]\n",
    "        NH_temp_100[i,61:120,:,:] = values2[:59,:,:]\n",
    "        NH_temp_100[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue\n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        tfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        t_files1 = tfile1[\"t\"] \n",
    "        t_data1 = t_files1.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values1 =  t_data1.values\n",
    "\n",
    "        tfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        t_files2 = tfile2[\"t\"] \n",
    "        t_data2 = t_files2.loc[dict(level=1000)] #designate lat/lon locale\n",
    "        values2 =  t_data2.values\n",
    "        \n",
    "        NH_temp_100[i,:61,:,:] = values1[90:,:,:]\n",
    "        NH_temp_100[i,61:120,:,:] = values2[:59,:,:]\n",
    "        NH_temp_100[i,121:,:,:] = values2[60:91,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d89b3-40e1-49b6-a39e-07877d3062c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(NH_temp_100, open(\"FullNH_temp_1979.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd3ca1-af7e-4b6e-9792-80b3289f56f9",
   "metadata": {},
   "source": [
    "## Imma throw in a really quick pair of cells to download data for eddy heat flux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70945c4a-f72d-48d9-9870-087c74b235ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100 = np.empty((44,152,361,1440))\n",
    "v100[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f177d-18c6-4f89-80af-4097a41c0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        vfile1 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i])+\".nc\")\n",
    "        v_files1 = vfile1[\"v\"] #next line will reduce this to the mean over th\n",
    "        v_data1 = v_files1.loc[dict(level=100)]\n",
    "        values1 =  v_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        vfile2 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i]+1)+\".nc\")\n",
    "        v_files2 = vfile2[\"v\"] #next line will reduce this to the mean \n",
    "        v_data2 = v_files2.loc[dict(level=100)]\n",
    "        values2 =  v_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        v100[i,:61] = values1[90:]\n",
    "        v100[i,61:] = values2[:91]\n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        #file 1 for start of year\n",
    "        vfile1 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i])+\".nc\")\n",
    "        v_files1 = vfile1[\"v\"] #next line will reduce this to the mean \n",
    "        v_data1 = v_files1.loc[dict(level=100)]\n",
    "        values1 =  v_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        vfile2 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i]+1)+\".nc\")\n",
    "        v_files2 = vfile2[\"v\"] #next line will reduce this to the mean\n",
    "        v_data2 = v_files2.loc[dict(level=100)]\n",
    "        values2 =  v_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        v100[i,:61] = values1[91:]\n",
    "        v100[i,61:120] = values2[:59]\n",
    "        v100[i,121:] = values2[60:91]\n",
    "        continue\n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        vfile1 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i])+\".nc\")\n",
    "        v_files1 = vfile1[\"v\"] #next line will reduce this to the mean over th\n",
    "        v_data1 = v_files1.loc[dict(level=100)]\n",
    "        values1 =  v_data1.values\n",
    "\n",
    "        vfile2 = xr.open_dataset(\"../era5/u/era5_uv_\"+str(year[i]+1)+\".nc\")\n",
    "        v_files2 = vfile2[\"v\"] #next line will reduce this to the mean \n",
    "        v_data2 = v_files2.loc[dict(level=100)]\n",
    "        values2 =  v_data2.values\n",
    "        \n",
    "        v100[i,:61] = values1[90:]\n",
    "        v100[i,61:120] = values2[:59]\n",
    "        v100[i,121:] = values2[60:91]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d29c9c-3aa2-41ec-85e2-d52cd8aa7876",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42ae2d-b273-48e7-933e-9f24d5529c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(v100, open(\"FullNH_v100_1979.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b2a41-f4de-4a72-abce-8089cfc5dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100 = np.empty((44,152,361,1440))\n",
    "t100[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b0eb-25e3-4a1d-b7ee-0229db5eb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    if year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] #next line will reduce this to the mean over th\n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        t100[i,:61] = values1[90:]\n",
    "        t100[i,61:] = values2[:91]\n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    if year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        #file 1 for start of year\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] #next line will reduce this to the mean \n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean\n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        t100[i,:61] = values1[91:]\n",
    "        t100[i,61:120] = values2[:59]\n",
    "        t100[i,121:] = values2[60:91]\n",
    "        continue\n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"t\"] #next line will reduce this to the mean over th\n",
    "        g_data1 = g_files1.loc[dict(level=100)]\n",
    "        values1 =  g_data1.values\n",
    "\n",
    "        gfile2 = xr.open_dataset(\"../era5/t/era5_t_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"t\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(level=100)]\n",
    "        values2 =  g_data2.values\n",
    "        \n",
    "        t100[i,:61] = values1[90:]\n",
    "        t100[i,61:120] = values2[:59]\n",
    "        t100[i,121:] = values2[60:91]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d98c94-9992-4938-ac3c-16f7af7d8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b025f-4731-426c-8f47-a5f5b355b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(t100, open(\"FullNH_t100_1979.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30508a02-72c3-48c3-b8a4-8d729c0334d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 August 2021 Environment",
   "language": "python",
   "name": "aug21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
